{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# The provided code snippet utilizes the pandas library to load and inspect a CSV file named\n",
        "#balanced_data.csv located in the /content/ directory of Google Colab.\n",
        "#The pd.read_csv() function reads the file into a DataFrame named df, which is a structured table of data.\n",
        "#The .shape attribute of the DataFrame is then called to retrieve a tuple\n",
        "#indicating the dimensions of the dataset, i.e., the number of rows and columns.\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/balanced_data.csv\")\n",
        "df.shape"
      ],
      "metadata": {
        "id": "H6DA4xSx2wGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "9N9ZWWl-2qwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data\n",
        "X = df.drop(\"FLAG\", axis=1)\n",
        "y = df[\"FLAG\"]"
      ],
      "metadata": {
        "id": "FLtd24XDytWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Uud3DKMxzDNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code below implements a stacking ensemble model combining Random Forest (RF), XGBoost (XGB), and a neural network (NN) to classify data and evaluate feature importance using SHAP, LIME, and ELI5. The Random Forest and XGBoost classifiers are initialized with specific hyperparameters tuned for optimal performance. The neural network is defined using Keras, comprising two dense hidden layers with ReLU activation and a sigmoid output layer for binary classification. These models are stacked, with ElasticNetCV serving as the meta-classifier, and the combined model is trained on a dataset divided into training (X_train, y_train) and testing (X_test, y_test) splits. The stacking model's performance is assessed using accuracy and a classification report. For explainability, SHAP is employed to analyze feature contributions globally, with results visualized as a bar plot. LIME provides local explanations for individual predictions by approximating the model's behavior for specific instances, showcasing the top contributing features. Lastly, ELI5's Permutation Importance is used to rank features by their impact on model predictions, presenting results in an interpretable format. This comprehensive approach integrates robust modeling with detailed explainability."
      ],
      "metadata": {
        "id": "aFAVSmlSrt0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define best hyperparameters for Random Forest\n",
        "rf_params = {'max_depth': 10, 'max_features': 0.3139538085100205,\n",
        "             'min_samples_leaf': 1.7307339350609041, 'min_samples_split': 14.052804288439336,\n",
        "             'n_estimators': 123}\n",
        "best_rf = RandomForestClassifier(**rf_params)\n",
        "\n",
        "# Define best hyperparameters for XGBoost\n",
        "xgb_params = {'colsample_bytree': 0.9729370974093524, 'learning_rate': 0.48726788904554486,\n",
        "              'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 105, 'subsample': 0.894412699905483}\n",
        "best_xgb = XGBClassifier(**xgb_params)\n",
        "\n",
        "# Define best hyperparameters for Neural Network\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "best_nn = create_model()\n",
        "\n",
        "# Create the stacking ensemble model with Elastic Net\n",
        "estimators = [\n",
        "    ('rf', best_rf),\n",
        "    ('xgb', best_xgb),\n",
        "    ('nn', best_nn)\n",
        "]\n",
        "stacking_model = StackingClassifier(estimators=estimators, final_estimator=ElasticNetCV())\n",
        "\n",
        "# Train the stacking model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the stacking model\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Explain feature importance using SHAP\n",
        "explainer = shap.Explainer(stacking_model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "\n",
        "# Explain feature importance using LIME\n",
        "lime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns.tolist(), class_names=['0', '1'], mode='classification')\n",
        "lime_exp = lime_explainer.explain_instance(X_test.iloc[0], stacking_model.predict_proba, num_features=10)\n",
        "lime_exp.show_in_notebook()\n",
        "\n",
        "# Explain feature importance using ELI5\n",
        "perm = PermutationImportance(stacking_model, random_state=42).fit(X_test, y_test)\n",
        "eli5.show_weights(perm, feature_names=X_test.columns.tolist())"
      ],
      "metadata": {
        "id": "yciTPLfGxrEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code evaluates the performance of a classification model using various metrics and visualizes the results through a confusion matrix heatmap. It calculates the Accuracy, Precision, Recall, and F1-Score based on predictions (y_pred) and true labels (y_test). The Confusion Matrix is computed to summarize prediction outcomes, showing the counts of true positives, true negatives, false positives, and false negatives. Metrics are printed for interpretation, and a heatmap of the confusion matrix is plotted using Seaborn for a clear visualization of the model's performance in distinguishing between classes. This approach provides both quantitative and visual insights into the classification results."
      ],
      "metadata": {
        "id": "rtZIZZwfsQC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "#auroc = roc_auc_score(y_test, best_xgb_clf.predict_proba(X_test)[:, 1])\n",
        "#mcc = matthews_corrcoef(y_test, y_pred)\n",
        "#brier_score = brier_score_loss(y_test, best_xgb_clf.predict_proba(X_test)[:, 1])\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {acc}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "#print(f\"AUROC: {auroc}\")\n",
        "#print(f\"MCC: {mcc}\")\n",
        "#print(f\"Brier Score: {brier_score}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "# Plot Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vxmUT2Bh2M7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}