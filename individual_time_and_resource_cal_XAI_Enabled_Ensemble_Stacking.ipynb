{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/balanced_data.csv\")\n",
        "df.shape"
      ],
      "metadata": {
        "id": "H6DA4xSx2wGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "9N9ZWWl-2qwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data\n",
        "X = df.drop(\"FLAG\", axis=1)\n",
        "y = df[\"FLAG\"]"
      ],
      "metadata": {
        "id": "FLtd24XDytWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Uud3DKMxzDNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code evaluates the performance and resource utilization of a Random Forest Classifier during training and testing. It tracks training time, memory usage, and CPU usage using the psutil library. The classifier is configured with specific hyperparameters like max_depth, max_features, min_samples_leaf, min_samples_split, and n_estimators. Before and after training, the memory usage and CPU activity are measured to compute the resources consumed. The training time is calculated by subtracting the start time from the end time. During testing, the model predicts on the test dataset (X_test), and the testing time is similarly recorded. Finally, the accuracy of the model is calculated using the accuracy_score function. Resource usage metrics, training/testing times, and accuracy are printed, providing a comprehensive understanding of the model's computational and predictive performance."
      ],
      "metadata": {
        "id": "FR-mobiHs8o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Return memory in MB and CPU percentage\n",
        "\n",
        "# Track training time and resource allocation\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "# Training the model\n",
        "model = RandomForestClassifier(\n",
        "    max_depth=10,\n",
        "    max_features=0.3139538085100205,\n",
        "    min_samples_leaf=int(1.7307339350609041),  # Ensure integer value\n",
        "    min_samples_split=int(14.052804288439336),  # Ensure integer value\n",
        "    n_estimators=123\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Memory Used during Training: {memory_used:.2f} MB\")\n",
        "print(f\"CPU Usage during Training: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Track testing time\n",
        "start_time = time.time()\n",
        "\n",
        "# Testing the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "w7Elkb5VuDkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This below code evaluates the performance of an XGBoost Classifier while monitoring resource usage and runtime efficiency during both training and testing phases. The XGBoost model is configured with optimized hyperparameters such as colsample_bytree, learning_rate, max_depth, min_child_weight, and others, and is designed for binary classification (objective='binary:logistic'). Using the psutil library, the script tracks memory usage (in MB) and CPU utilization (percentage) before and after training. The training time is measured using the time module, and the model is trained on X_train and y_train. Testing time is calculated similarly, and predictions are made on X_test. Finally, the script outputs the training time, testing time, memory consumption, CPU usage before and after training, and the accuracy of the model. This approach provides a comprehensive analysis of both the computational efficiency and predictive performance of the XGBoost model."
      ],
      "metadata": {
        "id": "QCvxF4pntZ8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Memory in MB, CPU %\n",
        "\n",
        "# Define best hyperparameters for XGBoost\n",
        "xgb_params = {\n",
        "    'colsample_bytree': 0.9729370974093524,\n",
        "    'learning_rate': 0.48726788904554486,\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 4,\n",
        "    'n_estimators': 105,\n",
        "    'subsample': 0.894412699905483,\n",
        "    'objective': 'binary:logistic',\n",
        "    'use_label_encoder': False\n",
        "}\n",
        "best_xgb = XGBClassifier(**xgb_params)\n",
        "\n",
        "# Track training time and resource usage\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "# Training the model\n",
        "best_xgb.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"XGBoost Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Memory Used during Training: {memory_used:.2f} MB\")\n",
        "print(f\"CPU Usage during Training: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Track testing time\n",
        "start_time = time.time()\n",
        "\n",
        "# Testing the model\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"XGBoost Testing Time: {testing_time:.2f} seconds\")\n",
        "print(f\"XGBoost Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "gSh6gBPqXwdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Normalize the data for neural network\n",
        "X_train = X_train / np.max(X_train, axis=0)\n",
        "X_test = X_test / np.max(X_train, axis=0)\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Memory in MB, CPU %\n",
        "\n",
        "# Define best hyperparameters for Neural Network\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "best_nn = create_model()\n",
        "\n",
        "# Track training time and resource usage\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "# Training the model\n",
        "history = best_nn.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"Neural Network Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Memory Used during Training: {memory_used:.2f} MB\")\n",
        "print(f\"CPU Usage during Training: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Track testing time\n",
        "start_time = time.time()\n",
        "\n",
        "# Testing the model\n",
        "y_pred = best_nn.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Neural Network Testing Time: {testing_time:.2f} seconds\")\n",
        "print(f\"Neural Network Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "wfbbP-6SYGg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "p-klTJw3YnC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install eli5"
      ],
      "metadata": {
        "id": "sMiRwYqKY8Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a stacking ensemble model that combines predictions from three machine learning models—Random Forest (RF), XGBoost (XGB), and a Neural Network (NN)—using an Elastic Net regression model as the meta-learner. Each model is trained with its own optimized hyperparameters, and resource usage (memory, CPU utilization) along with training and testing times are tracked using the psutil and time modules.\n",
        "\n",
        "Random Forest: Trained using hyperparameters like max_depth, n_estimators, and others. Predictions are made on X_test.\n",
        "XGBoost: Configured with advanced hyperparameters including learning_rate, max_depth, and subsample. After training, predictions are obtained.\n",
        "Neural Network: A sequential model with two hidden layers of 128 units each and ReLU activation is normalized for input features and trained over 10 epochs.\n",
        "Elastic Net Stacking: The predictions from RF, XGB, and NN are stacked as features and used to train an Elastic Net regression model. The meta-learner then predicts the final output."
      ],
      "metadata": {
        "id": "vf-FPt_FtsHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Memory in MB, CPU %\n",
        "\n",
        "# Normalize data for Neural Network\n",
        "X_train_nn = X_train / np.max(X_train, axis=0)\n",
        "X_test_nn = X_test / np.max(X_train, axis=0)\n",
        "\n",
        "# ============================= RANDOM FOREST =============================\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "rf_params = {\n",
        "    'max_depth': 10,\n",
        "    'max_features': 0.3139538085100205,\n",
        "    'min_samples_leaf': int(1.7307339350609041),\n",
        "    'min_samples_split': int(14.052804288439336),\n",
        "    'n_estimators': 123\n",
        "}\n",
        "rf_model = RandomForestClassifier(**rf_params)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"RF Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with Random Forest\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# ============================= XGBOOST =============================\n",
        "print(\"\\nTraining XGBoost...\")\n",
        "xgb_params = {\n",
        "    'colsample_bytree': 0.9729370974093524,\n",
        "    'learning_rate': 0.48726788904554486,\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 4,\n",
        "    'n_estimators': 105,\n",
        "    'subsample': 0.894412699905483,\n",
        "    'objective': 'binary:logistic',\n",
        "    'use_label_encoder': False\n",
        "}\n",
        "xgb_model = XGBClassifier(**xgb_params)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"XGBoost Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with XGBoost\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# ============================= NEURAL NETWORK =============================\n",
        "print(\"\\nTraining Neural Network...\")\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train_nn.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model()\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "nn_model.fit(X_train_nn, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"NN Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with Neural Network\n",
        "nn_pred_prob = nn_model.predict(X_test_nn)\n",
        "nn_pred = (nn_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# ============================= ELASTIC NET STACKING =============================\n",
        "print(\"\\nTraining Elastic Net for Stacking...\")\n",
        "# Combine predictions for stacking\n",
        "stacked_features = np.column_stack((rf_pred, xgb_pred, nn_pred))\n",
        "\n",
        "# Train Elastic Net\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "elastic_net.fit(stacked_features, y_test)  # Using test set predictions for demonstration\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"Elenet Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Final Predictions using Elastic Net\n",
        "final_pred = elastic_net.predict(stacked_features)\n",
        "final_pred_binary = (final_pred > 0.5).astype(int)\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "# Evaluate Ensemble Model\n",
        "accuracy = accuracy_score(y_test, final_pred_binary)\n",
        "print(f\"Ensemble Model Accuracy (Elastic Net): {accuracy:.2f}\")\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "aRzkV8vqaIHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Memory in MB, CPU %\n",
        "\n",
        "# Normalize data for Neural Network\n",
        "X_train_nn = X_train / np.max(X_train, axis=0)\n",
        "X_test_nn = X_test / np.max(X_train, axis=0)\n",
        "\n",
        "# ============================= RANDOM FOREST =============================\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "rf_params = {\n",
        "    'max_depth': 10,\n",
        "    'max_features': 0.3139538085100205,\n",
        "    'min_samples_leaf': int(1.7307339350609041),\n",
        "    'min_samples_split': int(14.052804288439336),\n",
        "    'n_estimators': 123\n",
        "}\n",
        "rf_model = RandomForestClassifier(**rf_params)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"RF Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with Random Forest\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# ============================= XGBOOST =============================\n",
        "print(\"\\nTraining XGBoost...\")\n",
        "xgb_params = {\n",
        "    'colsample_bytree': 0.9729370974093524,\n",
        "    'learning_rate': 0.48726788904554486,\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 4,\n",
        "    'n_estimators': 105,\n",
        "    'subsample': 0.894412699905483,\n",
        "    'objective': 'binary:logistic',\n",
        "    'use_label_encoder': False\n",
        "}\n",
        "xgb_model = XGBClassifier(**xgb_params)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"XGBoost Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with XGBoost\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# ============================= NEURAL NETWORK =============================\n",
        "print(\"\\nTraining Neural Network...\")\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train_nn.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model()\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "nn_model.fit(X_train_nn, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"NN Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with Neural Network\n",
        "nn_pred_prob = nn_model.predict(X_test_nn)\n",
        "nn_pred = (nn_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# ============================= ELASTIC NET STACKING =============================\n",
        "print(\"\\nTraining Elastic Net for Stacking...\")\n",
        "# Combine predictions for stacking\n",
        "stacked_features = np.column_stack((rf_pred, xgb_pred, nn_pred))\n",
        "\n",
        "# Train Elastic Net\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "elastic_net.fit(stacked_features, y_test)  # Using test set predictions for demonstration\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"Elenet Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Final Predictions using Elastic Net\n",
        "final_pred = elastic_net.predict(stacked_features)\n",
        "final_pred_binary = (final_pred > 0.5).astype(int)\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "# Evaluate Ensemble Model\n",
        "accuracy = accuracy_score(y_test, final_pred_binary)\n",
        "print(f\"Ensemble Model Accuracy (Elastic Net): {accuracy:.2f}\")\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "HlIeSctysKX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10 columns**"
      ],
      "metadata": {
        "id": "kQKYhVU6ZTdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of required columns\n",
        "selected_columns = [\n",
        "    \"Time Diff between first and last (Mins)\",\n",
        "    \" ERC20 uniq rec token name\",\n",
        "    \" ERC20 total Ether received\",\n",
        "    \"Avg min between received tnx\",\n",
        "    \"avg val received\",\n",
        "    \"Sent tnx\",\n",
        "    \"max value received \",\n",
        "    \"Received Tnx\",\n",
        "    \"total ether balance\",\n",
        "    \"total Ether sent\",\n",
        "    \"FLAG\"\n",
        "]\n",
        "\n",
        "# Load the dataset and select required columns\n",
        "data = pd.read_csv(\"/content/balanced_data.csv\")[selected_columns]"
      ],
      "metadata": {
        "id": "GEL3fuwdt7Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data\n",
        "X = data.drop(\"FLAG\", axis=1)\n",
        "y = data[\"FLAG\"]\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "t4PeiYh1w9mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "CR_97TCMxptF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Return memory in MB and CPU percentage\n",
        "\n",
        "# Track training time and resource allocation\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "# Training the model\n",
        "model = RandomForestClassifier(\n",
        "    max_depth=10,\n",
        "    max_features=0.3139538085100205,\n",
        "    min_samples_leaf=int(1.7307339350609041),  # Ensure integer value\n",
        "    min_samples_split=int(14.052804288439336),  # Ensure integer value\n",
        "    n_estimators=123\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Memory Used during Training: {memory_used:.2f} MB\")\n",
        "print(f\"CPU Usage during Training: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Track testing time\n",
        "start_time = time.time()\n",
        "\n",
        "# Testing the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "iMNhyfENy59h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Memory in MB, CPU %\n",
        "\n",
        "# Define best hyperparameters for XGBoost\n",
        "xgb_params = {\n",
        "    'colsample_bytree': 0.9729370974093524,\n",
        "    'learning_rate': 0.48726788904554486,\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 4,\n",
        "    'n_estimators': 105,\n",
        "    'subsample': 0.894412699905483,\n",
        "    'objective': 'binary:logistic',\n",
        "    'use_label_encoder': False\n",
        "}\n",
        "best_xgb = XGBClassifier(**xgb_params)\n",
        "\n",
        "# Track training time and resource usage\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "# Training the model\n",
        "best_xgb.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"XGBoost Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Memory Used during Training: {memory_used:.2f} MB\")\n",
        "print(f\"CPU Usage during Training: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Track testing time\n",
        "start_time = time.time()\n",
        "\n",
        "# Testing the model\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"XGBoost Testing Time: {testing_time:.2f} seconds\")\n",
        "print(f\"XGBoost Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "hmMOKbwFzHaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define best hyperparameters for Neural Network\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "best_nn = create_model()\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Memory in MB, CPU %\n",
        "\n",
        "# Measure training time and resource usage\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "# Train the model\n",
        "history = best_nn.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Memory Used during Training: {memory_used:.2f} MB\")\n",
        "print(f\"CPU Usage during Training: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Measure testing time\n",
        "start_time = time.time()\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_nn.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "AYxdA2WgzUD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to monitor resource usage\n",
        "def get_resource_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    cpu_percent = process.cpu_percent(interval=0.1)\n",
        "    return memory_info.rss / (1024 * 1024), cpu_percent  # Memory in MB, CPU %\n",
        "\n",
        "# Normalize data for Neural Network\n",
        "X_train_nn = X_train / np.max(X_train, axis=0)\n",
        "X_test_nn = X_test / np.max(X_train, axis=0)\n",
        "\n",
        "# ============================= RANDOM FOREST =============================\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "rf_params = {\n",
        "    'max_depth': 10,\n",
        "    'max_features': 0.3139538085100205,\n",
        "    'min_samples_leaf': int(1.7307339350609041),\n",
        "    'min_samples_split': int(14.052804288439336),\n",
        "    'n_estimators': 123\n",
        "}\n",
        "rf_model = RandomForestClassifier(**rf_params)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")\n",
        "\n",
        "print(f\"RF Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with Random Forest\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# ============================= XGBOOST =============================\n",
        "print(\"\\nTraining XGBoost...\")\n",
        "xgb_params = {\n",
        "    'colsample_bytree': 0.9729370974093524,\n",
        "    'learning_rate': 0.48726788904554486,\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 4,\n",
        "    'n_estimators': 105,\n",
        "    'subsample': 0.894412699905483,\n",
        "    'objective': 'binary:logistic',\n",
        "    'use_label_encoder': False\n",
        "}\n",
        "xgb_model = XGBClassifier(**xgb_params)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")\n",
        "\n",
        "print(f\"XGBoost Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with XGBoost\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# ============================= NEURAL NETWORK =============================\n",
        "print(\"\\nTraining Neural Network...\")\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train_nn.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model()\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "nn_model.fit(X_train_nn, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")\n",
        "\n",
        "print(f\"NN Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Predict with Neural Network\n",
        "nn_pred_prob = nn_model.predict(X_test_nn)\n",
        "nn_pred = (nn_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# ============================= ELASTIC NET STACKING =============================\n",
        "print(\"\\nTraining Elastic Net for Stacking...\")\n",
        "# Combine predictions for stacking\n",
        "stacked_features = np.column_stack((rf_pred, xgb_pred, nn_pred))\n",
        "\n",
        "# Train Elastic Net\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory, start_cpu = get_resource_usage()\n",
        "\n",
        "elastic_net.fit(stacked_features, y_test)  # Using test set predictions for demonstration\n",
        "\n",
        "end_memory, end_cpu = get_resource_usage()\n",
        "training_time = time.time() - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"Elenet Training Time: {training_time:.2f}s, Memory Used: {memory_used:.2f}MB, CPU: {start_cpu}% -> {end_cpu}%\")\n",
        "\n",
        "# Final Predictions using Elastic Net\n",
        "final_pred = elastic_net.predict(stacked_features)\n",
        "final_pred_binary = (final_pred > 0.5).astype(int)\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "# Evaluate Ensemble Model\n",
        "accuracy = accuracy_score(y_test, final_pred_binary)\n",
        "print(f\"Ensemble Model Accuracy (Elastic Net): {accuracy:.2f}\")\n",
        "print(f\"Testing Time: {testing_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "vmBUeMXuxeAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the Random Forest model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "# Perform bootstrapping\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train, y_train, test_size=0.3, random_state=i)\n",
        "\n",
        "    # Re-fit the Random Forest model on the bootstrapped dataset\n",
        "    rf_model.fit(X_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Display the results for each test sample\n",
        "print(\"Uncertainty Analysis for Test Samples (Random Forest):\")\n",
        "for idx, (mean, std) in enumerate(zip(mean_preds, std_preds)):\n",
        "    print(f\"Test Sample {idx + 1}: Mean Prediction = {mean:.4f}, Std Dev = {std:.4f}\")\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the overall averages\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")\n",
        "\n",
        "# Visualize the uncertainty (standard deviation) as a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(std_preds)), std_preds, alpha=0.7, label=\"Random Forest Uncertainty (Std Dev)\")\n",
        "plt.title(\"Uncertainty in Random Forest Predictions (Standard Deviation)\")\n",
        "plt.ylabel(\"Standard Deviation of Predictions\")\n",
        "plt.xlabel(\"Test Samples\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GBfPfhLIOOSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "# Perform bootstrapping\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train, y_train, test_size=0.3, random_state=i)\n",
        "\n",
        "    # Re-fit the XGBoost model on the bootstrapped dataset\n",
        "    xgb_model.fit(X_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the overall averages\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")\n",
        "\n",
        "# Visualize the uncertainty (standard deviation) as a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(std_preds)), std_preds, alpha=0.7, label=\"XGBoost Uncertainty (Std Dev)\")\n",
        "plt.title(\"Uncertainty in XGBoost Predictions (Standard Deviation)\")\n",
        "plt.ylabel(\"Standard Deviation of Predictions\")\n",
        "plt.xlabel(\"Test Samples\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qdn_yObgQvFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "JRjLcDifSx_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow keras"
      ],
      "metadata": {
        "id": "mSS5TDSYTm7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow keras"
      ],
      "metadata": {
        "id": "3XmRj-yjW-yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "Ea4ly5lnXnFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)  # Ensure it's 2.x or above"
      ],
      "metadata": {
        "id": "kJqbjrp5Xrnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier  # Correct import\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "best_nn = create_model()\n",
        "\n",
        "# Train the model\n",
        "nn_model = best_nn.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train, y_train, test_size=0.3, random_state=i)\n",
        "\n",
        "    # Fit the neural network on the bootstrapped dataset\n",
        "    nn_model.fit(X_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = nn_model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the overall averages\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")"
      ],
      "metadata": {
        "id": "Yu-5tDs1X0pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize and train the model\n",
        "best_nn = create_model()\n",
        "best_nn.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=i)\n",
        "\n",
        "    # Reinitialize and fit the neural network on the bootstrapped dataset\n",
        "    bootstrap_nn = create_model()  # Create a fresh model for each bootstrap\n",
        "    bootstrap_nn.fit(X_bootstrap, y_bootstrap, epochs=10, batch_size=64, verbose=0)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = bootstrap_nn.predict(X_test).flatten()\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the overall averages\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")"
      ],
      "metadata": {
        "id": "YPYIw3Q4avIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# Define Random Forest and XGBoost models\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the Neural Network model\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model()\n",
        "\n",
        "# Train Neural Network on the training set\n",
        "nn_model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=0)\n",
        "\n",
        "# Generate predictions from the neural network to use in stacking\n",
        "nn_preds_train = nn_model.predict(X_train).flatten()\n",
        "nn_preds_test = nn_model.predict(X_test).flatten()\n",
        "\n",
        "# Add NN predictions as a feature to X_train and X_test\n",
        "X_train_stack = np.hstack([X_train, nn_preds_train.reshape(-1, 1)])\n",
        "X_test_stack = np.hstack([X_test, nn_preds_test.reshape(-1, 1)])\n",
        "\n",
        "# Combine RF and XGB predictions as additional features\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "rf_preds_train = rf_model.predict_proba(X_train)[:, 1]\n",
        "rf_preds_test = rf_model.predict_proba(X_test)[:, 1]\n",
        "xgb_preds_train = xgb_model.predict_proba(X_train)[:, 1]\n",
        "xgb_preds_test = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Create the final stacking dataset\n",
        "X_train_final = np.hstack([X_train_stack, rf_preds_train.reshape(-1, 1), xgb_preds_train.reshape(-1, 1)])\n",
        "X_test_final = np.hstack([X_test_stack, rf_preds_test.reshape(-1, 1), xgb_preds_test.reshape(-1, 1)])\n",
        "\n",
        "# Meta-model (ElasticNet)\n",
        "meta_model = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)\n",
        "meta_model.fit(X_train_final, y_train)\n",
        "\n",
        "# Predict using the stacking model\n",
        "stacking_preds = meta_model.predict(X_test_final)\n",
        "stacking_preds_binary = (stacking_preds > 0.5).astype(int)\n",
        "\n",
        "# Evaluate accuracy of the stacking model\n",
        "accuracy = accuracy_score(y_test, stacking_preds_binary)\n",
        "print(f\"Stacking Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train_final, y_train, test_size=0.3, random_state=i)\n",
        "\n",
        "    # Fit the meta-model on the bootstrapped dataset\n",
        "    meta_model.fit(X_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = meta_model.predict(X_test_final)\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")\n"
      ],
      "metadata": {
        "id": "Mw0D84oae4nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **17 columns**"
      ],
      "metadata": {
        "id": "NYDLlW0cf8G2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the Random Forest model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "# Perform bootstrapping\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train, y_train, test_size=0.3, random_state=i)\n",
        "\n",
        "    # Re-fit the Random Forest model on the bootstrapped dataset\n",
        "    rf_model.fit(X_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Display the results for each test sample\n",
        "print(\"Uncertainty Analysis for Test Samples (Random Forest):\")\n",
        "for idx, (mean, std) in enumerate(zip(mean_preds, std_preds)):\n",
        "    print(f\"Test Sample {idx + 1}: Mean Prediction = {mean:.4f}, Std Dev = {std:.4f}\")\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the overall averages\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")\n",
        "\n",
        "# Visualize the uncertainty (standard deviation) as a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(std_preds)), std_preds, alpha=0.7, label=\"Random Forest Uncertainty (Std Dev)\")\n",
        "plt.title(\"Uncertainty in Random Forest Predictions (Standard Deviation)\")\n",
        "plt.ylabel(\"Standard Deviation of Predictions\")\n",
        "plt.xlabel(\"Test Samples\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FUxWUETmfc6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "# Perform bootstrapping\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train, y_train, test_size=0.3, random_state=i)\n",
        "\n",
        "    # Re-fit the XGBoost model on the bootstrapped dataset\n",
        "    xgb_model.fit(X_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the overall averages\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")\n",
        "\n",
        "# Visualize the uncertainty (standard deviation) as a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(std_preds)), std_preds, alpha=0.7, label=\"XGBoost Uncertainty (Std Dev)\")\n",
        "plt.title(\"Uncertainty in XGBoost Predictions (Standard Deviation)\")\n",
        "plt.ylabel(\"Standard Deviation of Predictions\")\n",
        "plt.xlabel(\"Test Samples\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "419Xkl6mfrQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize and train the model\n",
        "best_nn = create_model()\n",
        "best_nn.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=i)\n",
        "\n",
        "    # Reinitialize and fit the neural network on the bootstrapped dataset\n",
        "    bootstrap_nn = create_model()  # Create a fresh model for each bootstrap\n",
        "    bootstrap_nn.fit(X_bootstrap, y_bootstrap, epochs=10, batch_size=64, verbose=0)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = bootstrap_nn.predict(X_test).flatten()\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the overall averages\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")"
      ],
      "metadata": {
        "id": "M_Wp5S6VfxK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# Define Random Forest and XGBoost models\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the Neural Network model\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model()\n",
        "\n",
        "# Train Neural Network on the training set\n",
        "nn_model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=0)\n",
        "\n",
        "# Generate predictions from the neural network to use in stacking\n",
        "nn_preds_train = nn_model.predict(X_train).flatten()\n",
        "nn_preds_test = nn_model.predict(X_test).flatten()\n",
        "\n",
        "# Add NN predictions as a feature to X_train and X_test\n",
        "X_train_stack = np.hstack([X_train, nn_preds_train.reshape(-1, 1)])\n",
        "X_test_stack = np.hstack([X_test, nn_preds_test.reshape(-1, 1)])\n",
        "\n",
        "# Combine RF and XGB predictions as additional features\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "rf_preds_train = rf_model.predict_proba(X_train)[:, 1]\n",
        "rf_preds_test = rf_model.predict_proba(X_test)[:, 1]\n",
        "xgb_preds_train = xgb_model.predict_proba(X_train)[:, 1]\n",
        "xgb_preds_test = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Create the final stacking dataset\n",
        "X_train_final = np.hstack([X_train_stack, rf_preds_train.reshape(-1, 1), xgb_preds_train.reshape(-1, 1)])\n",
        "X_test_final = np.hstack([X_test_stack, rf_preds_test.reshape(-1, 1), xgb_preds_test.reshape(-1, 1)])\n",
        "\n",
        "# Meta-model (ElasticNet)\n",
        "meta_model = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)\n",
        "meta_model.fit(X_train_final, y_train)\n",
        "\n",
        "# Predict using the stacking model\n",
        "stacking_preds = meta_model.predict(X_test_final)\n",
        "stacking_preds_binary = (stacking_preds > 0.5).astype(int)\n",
        "\n",
        "# Evaluate accuracy of the stacking model\n",
        "accuracy = accuracy_score(y_test, stacking_preds_binary)\n",
        "print(f\"Stacking Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Perform bootstrapping for uncertainty analysis\n",
        "n_bootstraps = 100\n",
        "bootstrap_preds = np.zeros((n_bootstraps, X_test.shape[0]))\n",
        "\n",
        "for i in range(n_bootstraps):\n",
        "    # Sample with replacement from the training data\n",
        "    X_bootstrap, _, y_bootstrap, _ = train_test_split(X_train_final, y_train, test_size=0.3, random_state=i)\n",
        "\n",
        "    # Fit the meta-model on the bootstrapped dataset\n",
        "    meta_model.fit(X_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Get predictions for the test set\n",
        "    bootstrap_preds[i] = meta_model.predict(X_test_final)\n",
        "\n",
        "# Calculate the mean and standard deviation of predictions\n",
        "mean_preds = np.mean(bootstrap_preds, axis=0)  # Mean predictions\n",
        "std_preds = np.std(bootstrap_preds, axis=0)    # Standard deviation (uncertainty) of predictions\n",
        "\n",
        "# Calculate the overall average of mean predictions and standard deviations\n",
        "average_mean_prediction = np.mean(mean_preds)\n",
        "average_std_deviation = np.mean(std_preds)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Average Mean Prediction: {average_mean_prediction:.4f}\")\n",
        "print(f\"Average Standard Deviation: {average_std_deviation:.4f}\")"
      ],
      "metadata": {
        "id": "cprGqYrFf29s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}